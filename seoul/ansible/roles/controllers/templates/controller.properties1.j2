node.id=101
process.roles=controller
controller.listener.names=CONTROLLER
controller.quorum.voters=101@{{cp1_controller1_a_private_ip}}:9093,102@{{cp1_controller2_a_private_ip}}:9093,103@{{cp1_controller3_a_private_ip}}:9093

listeners=CONTROLLER://:9093
listener.security.protocol.map=CONTROLLER:PLAINTEXT

authorizer.class.name=io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
confluent.metadata.server.kraft.controller.enabled=true
confluent.metadata.server.token.key.path=/var/ssl/private/tokenKeypair.pem
confluent.authorizer.access.rule.providers=CONFLUENT
super.users=User:admin;User:ANONYMOUS

confluent.metadata.bootstrap.servers={{cp1_broker1_a_public_ip}}:9092,{{cp1_broker2_a_public_ip}}:9092,{{cp1_broker3_a_public_ip}}:9092
confluent.metadata.security.protocol=SASL_PLAINTEXT
confluent.metadata.sasl.mechanism=SCRAM-SHA-512
confluent.metadata.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";

metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter
confluent.metrics.reporter.bootstrap.servers={{cp1_broker1_a_public_ip}}:9092,{{cp1_broker2_a_public_ip}}:9092,{{cp1_broker3_a_public_ip}}:9092
confluent.metrics.reporter.security.protocol=SASL_PLAINTEXT
confluent.metrics.reporter.sasl.mechanism=SCRAM-SHA-512
confluent.metrics.reporter.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";

confluent.security.event.logger.enable=true
confluent.security.event.logger.exporter.kafka.topic.create=false
confluent.security.event.logger.exporter.kafka.bootstrap.servers={{docker_compose_public_ip}}:29092,{{docker_compose_public_ip}}:39092,{{docker_compose_public_ip}}:49092
confluent.security.event.logger.exporter.kafka.security.protocol=PLAINTEXT
# confluent.security.event.logger.exporter.kafka.sasl.mechanism=SCRAM-SHA-512
# confluent.security.event.logger.exporter.kafka.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";

log.cleanup.policy=delete
log.dirs=/appData/data/kafka-controller
log.retention.check.interval.ms=300000
log.retention.hours=72
log.segment.bytes=1073741824

num.io.threads=8
num.network.threads=3
num.partitions=3
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=3

socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
socket.send.buffer.bytes=102400
transaction.state.log.min.isr=2
transaction.state.log.replication.factor=3

confluent.balancer.enable=false

# metric.reporters=io.confluent.telemetry.reporter.TelemetryReporter
# confluent.telemetry.exporter._c3.type=http
# confluent.telemetry.exporter._c3.enabled=true
# confluent.telemetry.exporter._c3.metrics.include=io.confluent.kafka.server.request.(?!.delta).|io.confluent.kafka.server.server.broker.state|io.confluent.kafka.server.replica.manager.leader.count|io.confluent.kafka.server.request.queue.size|io.confluent.kafka.server.broker.topic.failed.produce.requests.rate.1.min|io.confluent.kafka.server.tier.archiver.total.lag|io.confluent.kafka.server.request.total.time.ms.p99|io.confluent.kafka.server.broker.topic.failed.fetch.requests.rate.1.min|io.confluent.kafka.server.broker.topic.total.fetch.requests.rate.1.min|io.confluent.kafka.server.partition.caught.up.replicas.count|io.confluent.kafka.server.partition.observer.replicas.count|io.confluent.kafka.server.tier.tasks.num.partitions.in.error|io.confluent.kafka.server.broker.topic.bytes.out.rate.1.min|io.confluent.kafka.server.request.total.time.ms.p95|io.confluent.kafka.server.controller.active.controller.count|io.confluent.kafka.server.session.expire.listener.zookeeper.disconnects.total|io.confluent.kafka.server.request.total.time.ms.p999|io.confluent.kafka.server.controller.active.broker.count|io.confluent.kafka.server.request.handler.pool.request.handler.avg.idle.percent.rate.1.min|io.confluent.kafka.server.session.expire.listener.zookeeper.disconnects.rate.1.min|io.confluent.kafka.server.controller.unclean.leader.elections.rate.1.min|io.confluent.kafka.server.replica.manager.partition.count|io.confluent.kafka.server.controller.unclean.leader.elections.total|io.confluent.kafka.server.partition.replicas.count|io.confluent.kafka.server.broker.topic.total.produce.requests.rate.1.min|io.confluent.kafka.server.controller.offline.partitions.count|io.confluent.kafka.server.socket.server.network.processor.avg.idle.percent|io.confluent.kafka.server.partition.under.replicated|io.confluent.kafka.server.log.log.start.offset|io.confluent.kafka.server.log.tier.size|io.confluent.kafka.server.log.size|io.confluent.kafka.server.tier.fetcher.bytes.fetched.total|io.confluent.kafka.server.request.total.time.ms.p50|io.confluent.kafka.server.tenant.consumer.lag.offsets|io.confluent.kafka.server.session.expire.listener.zookeeper.expires.rate.1.min|io.confluent.kafka.server.log.log.end.offset|io.confluent.kafka.server.broker.topic.bytes.in.rate.1.min|io.confluent.kafka.server.partition.under.min.isr|io.confluent.kafka.server.partition.in.sync.replicas.count|io.confluent.telemetry.http.exporter.batches.dropped|io.confluent.telemetry.http.exporter.items.total|io.confluent.telemetry.http.exporter.items.succeeded|io.confluent.telemetry.http.exporter.send.time.total.millis|io.confluent.kafka.server.controller.leader.election.rate.(?!.delta).|io.confluent.telemetry.http.exporter.batches.failed
# confluent.telemetry.exporter._c3.client.base.url=http://control-center11.tg.com:9090/api/v1/otlp
# confluent.telemetry.exporter._c3.client.compression=gzip
# confluent.telemetry.exporter._c3.api.key=dummy
# confluent.telemetry.exporter._c3.api.secret=dummy
# confluent.telemetry.exporter._c3.buffer.pending.batches.max=80
# confluent.telemetry.exporter._c3.buffer.batch.items.max=4000
# confluent.telemetry.exporter._c3.buffer.inflight.submissions.max=10
# confluent.telemetry.metrics.collector.interval.ms=60000
# confluent.telemetry.remoteconfig._confluent.enabled=false
# confluent.consumer.lag.emitter.enabled=true
